# chapter05. RNN

- 지금까지 살펴본 신경망은 **피드포워드** 유형의 신경망으로 단방향을 가진 신경망입니다.

  - 다시 말해, 입력 신호가 다음 층으로 전달되고 그 신호를 받은 층이 다음 층으로 전달하는 방식으로 한 방향으로만 전달됩니다.

- 피드포워드 신경망은 구성이 단순해 구조를 이해하기 쉽고 많은 문제에 응용할 수 있습니다. 하지만 시계열 데이터를 잘 다루지 못한다는 단점이 있습니다.

  - 더 정확하게, 단순한 피드포워드 신경망에서는 시계열 데이터의 성질(패턴)을 잘 학습할 수 없습니다. 그래서 등장한 것이 **순환 신경망(RNN)**입니다.

- 이번 장에서는 피드포워드 신경망의 문제점을 지적하고, RNN이 그 문제를 훌륭하게 해결할 수 있음을 설명합니다.

- 또한, RNN의 구조를 차분히 시간을 들여 설명하고 Python으로 구현해볼 것입니다.

## 5.1 확률과 언어 모델

- 이번 절에서는 RNN 이야기를 시작하기 전 준비 과정으로 앞 장의 word2vec을 복습하겠습니다.

- 그런 다음 자연어에 관한 현상을 '확률'을 사용해 기술하고 마지막에 언어를 확률로 다루는 '언어 모델'에 대해 설명합니다.

### 5.1.1 word2vec을 확률 관점에서 바라보다.

- 그럼 word2vec의 CBOW 모델부터 복습하겠습니다. 여기에서는 w<sub>T</sub>라는 단어열로 표현되는 말뭉치에서 t번째 단어를 '타깃'으로 그 전후 단어를 '맥락'으로 취급하겠습니다.

#### Warning

> 이 책에서 타깃은 '중앙 단어'를, 맥락은 타깃의 '주변 단어'를 가리킵니다.

- 이 때 CBOW 모델은 타깃 단어 전후 맥락으로부터 타깃을 추측하는 일을 수행합니다.

<img src="README.assets/fig 5-1.png" alt="fig 5-1" style="zoom:50%;" />

- 그럼 맥락이 주어졌을 때 타깃이 w<sub>T</sub>일 확률을 수식으로 구현하면 다음과 같습니다.

<img src="README.assets/e 5-1.png" alt="e 5-1" style="zoom:50%;" />

- CBOW 모델은 위 식의 사후 확률을 모델링하는데 여기서 사후 확률은 '맥락이 주어졌을 때 타깃 단어를 찾을 확률'을 의미합니다.

  - 이것이 윈도우 크기가 1일 때의 CBOW 모델입니다.

- 그런데 지금까지 맥락을 항상 좌우 대칭으로 생각했습니다. 이번에는 맥락을 왼쪽 윈도우로만 한정하는 다음과 같은 경우를 생각해보겠습니다.

<img src="README.assets/fig 5-2.png" alt="fig 5-2" style="zoom:50%;" />

- 이와 같이 왼쪽 두 단어만을 맥락으로 생각했을 때 CBOW 모델이 출력할 확률은 다음과 같습니다.

<img src="README.assets/e 5-2.png" alt="e 5-2" style="zoom:50%;" />

#### Note

> word2vec에서 맥락의 윈도우 크기는 하이퍼파라미터로 임의의 값을 설정할 수 있습니다.
>
> 이번 예에서는 윈도우 크기를 '왼쪽 2, 오른쪽 0'처럼 좌우 비대칭으로 설정했습니다.
>
> 이렇게 설정한 이유는 나중에 '언어 모델'에서 이야기하겠습니다.

- 그런데 위 식 표기를 사용하면 CBOW 모델이 다루는 손실함수를 교차 엔트로피 오차에 의해 다음과 같이 유도할 수 있습니다.

<img src="README.assets/e 5-3.png" alt="e 5-3" style="zoom:50%;" />

- CBOW 모델의 학습으로 수행하는 일은 위 식의 손실 함수(정확히는 말뭉치 전체의 손실 함수의 총합)를 최소화하는 가중치 매개변수를 찾는 것입니다.

  - 이러한 가중치 매개변수가 발견되면 CBOW 모델을 맥락에서 타깃을 더 정확하게 추측할 수 있습니다.

- 이처럼 CBOW 모델을 학습시키는 목적은 맥락에서 타깃을 정확하게 추측하는 것으로 학습 부산물로 단어의 의미가 인코딩된 '단어의 분산표현'을 얻을 수 있습니다.

- 한편 CBOW 모델의 목적인 '맥락에서 타깃을 추측하는 것'은 어디에 이용될 수 있는가와 위 식의 확률의 실용적인 쓰임이 바로 '언어 모델'에서 나타나게 됩니다.

### 5.1.2 언어 모델

- **언어 모델**은 단어 나열에 확률을 부여합니다.

  - 특정 단어의 시퀀스에 대해서, 그 시퀀스가 일어날 가능성이 어느 정도인지 즉, 얼마나 자연스러운 단어 순서인지를 확률로 평가합니다.

  - 예를 들어 'you say goodbye'라는 단어 시퀀스는 높은 확률을, 'you say good die'는 낮은 확률을 출력하는 모델입니다.

- 이 언어 모델은 다양하게 응용되는데 기계 번역과 음성 인식이 대표적인 예입니다.

  - 음성 인식의 경우 사람의 음성에서 몇 개의 문장을 후보로 생성하고 언어 모델을 사용해 후보 문장이 '문장으로서 자연스러운지'를 기준으로 순서를 매깁니다.

  - 또한 언어 모델이 단어 순서의 자연스러움을 확률적으로 평가할 수 있으므로 새로운 문장을 생성하는 용도로도 이용할 수 있습니다.

    - 그 확률 분포에 따라 다음으로 적합한 단어를 '자아낼(샘플링)' 수 있기 때문으로 이는 '7장 RNN을 사용한 문장 생성'에서 배울 것입니다.

- 그러면 언어 모델을 수식으로 설명해보겠습니다. 여기서는 w<sub>m</sub>까지 m개의 단어로 된 문장을 생각해보겠습니다.

- 이 때 단어가 w<sub>1</sub>에서 w<sub>m</sub> 순서로 나타날 확률을 P(w<sub>1</sub>, ..., w<sub>m</sub>)이라고 할 수 있고 이는 동시에 일어나므로 동시 확률입니다. 이 동시 확률 P는 사후 확률을 사용해 다음과 같이 분해할 수 있습니다.

<img src="README.assets/e 5-4.png" alt="e 5-4" style="zoom:50%;" />

- 위 식에서 파이 기호는 모든 원소를 곱하는 '총곱'을 의미하며 동시 확률을 사후 확률의 총곱으로 나타낼 수 있습니다.

- 위 식의 경과는 확률의 **곱셈정리**에서 유도한 것으로 곱셈정리에 대한 간단한 설명 후에 이를 도출하는 과정을 설명하겠습니다. 우선 곱셈 정리는 다음과 같습니다.

<img src="README.assets/e 5-5.png" alt="e 5-5" style="zoom:50%;" />

- 위와 같은 곱셈정리는 확률론에서 가장 중요한 정리로 'A와 B가 모두 일어날 확률 P(A, B)는 B가 일어날 확률 P(B)에 B가 일어난 후 A가 일어날 확률 P(A|B)를 곱한 것과 같다.'는 것입니다.

#### Warning

> 확률 P(A, B)는 P(A, B) = P(B|A)P(A)로 분해할 수 있습니다.
>
> 즉, A와 B 중 어느 것을 사후 확률의 조건으로 할지에 따라 두 가지 식 표현 방법이 존재합니다.

- 이 곱셈정리를 사용하면 m개의 단어의 동시 확률을 사후 확률로 나타낼 수 있는데 이 때 수행하는 식 변형을 알기 쉽게 나타내면 다음과 같습니다.

<img src="README.assets/e 5-6.png" alt="e 5-6" style="zoom:50%;" />

- 여기에서 w<sub>m</sub>을 제외한 나머지를 A로 표현했는데 이 때 P(A) 역시 다음과 같은 식 변형을 할 수 있습니다.

<img src="README.assets/e 5-7.png" alt="e 5-7" style="zoom:50%;" />

- 이처럼 단어 시퀀스를 하나씩 줄여가면서 매번 사후 확률로 분해하고 이를 반복해 [식 5.4]를 이끌어낼 수 있습니다.

- 이 때 [식 5.4]에서 알 수 있듯이 목적으로 하는 동시 확률은 사후 확률의 총곱으로 대표할 수 있고 여기서 주목할 것은 이 사후 확률은 타깃 단어보다 왼쪽에 있는 모든 단어를 맥락(조건)으로 했을 때의 확률이라는 것입니다.

<img src="README.assets/fig 5-3.png" alt="fig 5-3" style="zoom:50%;" />

- 즉, 이를 정리하면 우리의 목표인 사후 확률을 구하는 것으로 이를 계산할 수 있다면 언어 모델의 동시 확률을 구할 수 있습니다.

#### Note

> P(w<sub>t</sub>|others)를 나타내는 모델은 **조건부 언어 모델**이라고 합니다.
>
> 한편 해당 확률을 나타내는 모델을 '언어 모델'이라 하는 경우도 많이 볼 수 있습니다.

### 5.1.3 CBOW 모델을 언어 모델로?

- 그렇다면 word2vec의 CBOW 모델을 언어 모델에 적용하는 방법은 무엇이 있을까요?

  - 이는 맥락의 크기를 특정 값으로 한정하여 근사적으로 나타낼 수 있는데 수식으로는 다음과 같습니다.

  <img src="README.assets/e 5-8.png" alt="e 5-8" style="zoom:50%;" />

  - 여기에서는 맥락을 왼쪽 두 개의 단어로 한정한 것으로 CBOW 모델에 따라(정확히는 모델의 사후 확률) 근사적으로 나타낼 수 있습니다.

#### Note

> 머신러닝이나 통계학에서는 **마르코프 연쇄** 또는 **마르코프 모델**이라는 말을 자주 듣습니다.
>
> 마르코프 연쇄란 미래의 상태가 현재 상태에만 의존해 결정되는 것을 의미합니다.
>
> 또한, 이 사상의 확률을 '그 직전' N개의 사건에만 의존할 때 이를 'N층 마르코프 연쇄'라고 합니다.
>
> 이번 예는 직전 2개의 단어에만 의존해 다음 단어가 정해지는 모델이므로 '2층 마르코프 연쇄라고 할 수 있습니다.

- 위 식에서는 맥락으로 두 개의 단어를 이용했지만 맥락의 크기는 임의의 길이로 설정할 수 있습니다.

- 하지만 결국 특정 길이로 '고정'되는데 예를 들어 왼쪽 10개의 단어를 맥락으로 하는 CBOW 모델은 그 맥락보다 더 왼쪽의 단어 정보를 무시합니다.

  - 이러한 경우 문제가 발생하는데 그 예가 다음과 같습니다.

<img src="README.assets/fig 5-4.png" alt="fig 5-4" style="zoom:50%;" />

- 위 예시의 문제의 맥락을 고려하면 정답은 'Tom(he)'이지만 이를 알기 위해 타깃보다 18번째 앞의 단어 'Tom'을 기억해야 합니다.

  - 따라서 10개의 맥락을 가지는 CBOW 모델은 이를 제대로 답할 수 없습니다.

- 그렇다고 CBOW 모델의 맥락 크기를 계속해서 키워나가도 CBOW 모델이 맥락 안의 단어 순서를 무시한다는 한계가 있습니다.

#### Note

> CBOW는 continuous bag-of-words의 약자로 bag-of-words는 '가방 안의 단어'를 의미하는데 이 때 단어의 '순서'는 무시된다는 뜻이 내포되어 있습니다.

- 맥락의 단어 순서가 무시되는 구체적인 예로 맥락으로 2개의 단어를 다루는 CBOW 모델이 두 개의 단어 벡터의 '합'이 은닉층으로 넘어오는 과정은 다음과 같습니다.

<img src="README.assets/fig 5-5.png" alt="fig 5-5" style="zoom:50%;" />

- 위 그림의 왼쪽과 같이 CBOW 모델의 은닉층에서는 단어 벡터들이 더해지므로 맥락의 단어 순서는 무시됩니다.

- 이상적으로는 맥락의 단어 순서도 고려한 모델이 바람직한데 이는 위 그림의 오른쪽 모델과 같습니다.

  - 오른쪽 모델은 단어 벡터를 은닉층에서 **연결**하는 방식을 사용하는데 실제 신경 확률론적 언어 모델에서 제안한 모델은 이 방식을 취합니다.

  - 그러나 연결하는 방식은 맥락의 크기에 비례해 가중치 변수가 늘어나는 단점을 불러옵니다.

- 따라서 이러한 문제를 해결하기 위해 순환 신경망, 즉 RNN이 등장했습니다.

- RNN은 맥락이 아무리 길더라도 그 맥락의 정보를 기억하는 매커니즘을 갖추고 있습니다. 그래서 RNN을 사용하면 아무리 긴 시계열 데이터에도 대응할 수 있습니다.

#### Warning

> word2vec은 단어의 분산 표현을 얻을 목적으로 고안된 기법으로 이를 언어 모델로 사용하는 경우는 보통 잘 없습니다.
>
> 여기에서는 RNN의 매력을 알아가기 위해 word2vec의 CBOW 모델을 억지고 언어 모델에 적용했을 뿐입니다.
>
> 특히 word2vec의 RNN보다 늦게 제안되었는데 RNN 언어 모델이 단어의 분산 표현을 얻을 수 있지만 어휘 수 증가에 따른 대응이나 단어의 분산 표현의 '질' 개선을 위해 word2vec을 제안되었습니다.
