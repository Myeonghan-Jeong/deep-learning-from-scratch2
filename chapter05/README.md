# chapter05. RNN

- 지금까지 살펴본 신경망은 **피드포워드** 유형의 신경망으로 단방향을 가진 신경망입니다.

  - 다시 말해, 입력 신호가 다음 층으로 전달되고 그 신호를 받은 층이 다음 층으로 전달하는 방식으로 한 방향으로만 전달됩니다.

- 피드포워드 신경망은 구성이 단순해 구조를 이해하기 쉽고 많은 문제에 응용할 수 있습니다. 하지만 시계열 데이터를 잘 다루지 못한다는 단점이 있습니다.

  - 더 정확하게, 단순한 피드포워드 신경망에서는 시계열 데이터의 성질(패턴)을 잘 학습할 수 없습니다. 그래서 등장한 것이 **순환 신경망(RNN)**입니다.

- 이번 장에서는 피드포워드 신경망의 문제점을 지적하고, RNN이 그 문제를 훌륭하게 해결할 수 있음을 설명합니다.

- 또한, RNN의 구조를 차분히 시간을 들여 설명하고 Python으로 구현해볼 것입니다.

## 5.1 확률과 언어 모델

- 이번 절에서는 RNN 이야기를 시작하기 전 준비 과정으로 앞 장의 word2vec을 복습하겠습니다.

- 그런 다음 자연어에 관한 현상을 '확률'을 사용해 기술하고 마지막에 언어를 확률로 다루는 '언어 모델'에 대해 설명합니다.

### 5.1.1 word2vec을 확률 관점에서 바라보다.

- 그럼 word2vec의 CBOW 모델부터 복습하겠습니다. 여기에서는 w<sub>T</sub>라는 단어열로 표현되는 말뭉치에서 t번째 단어를 '타깃'으로 그 전후 단어를 '맥락'으로 취급하겠습니다.

#### Warning

> 이 책에서 타깃은 '중앙 단어'를, 맥락은 타깃의 '주변 단어'를 가리킵니다.

- 이 때 CBOW 모델은 타깃 단어 전후 맥락으로부터 타깃을 추측하는 일을 수행합니다.

<img src="README.assets/fig 5-1.png" alt="fig 5-1" style="zoom:50%;" />

- 그럼 맥락이 주어졌을 때 타깃이 w<sub>T</sub>일 확률을 수식으로 구현하면 다음과 같습니다.

<img src="README.assets/e 5-1.png" alt="e 5-1" style="zoom:50%;" />

- CBOW 모델은 위 식의 사후 확률을 모델링하는데 여기서 사후 확률은 '맥락이 주어졌을 때 타깃 단어를 찾을 확률'을 의미합니다.

  - 이것이 윈도우 크기가 1일 때의 CBOW 모델입니다.

- 그런데 지금까지 맥락을 항상 좌우 대칭으로 생각했습니다. 이번에는 맥락을 왼쪽 윈도우로만 한정하는 다음과 같은 경우를 생각해보겠습니다.

<img src="README.assets/fig 5-2.png" alt="fig 5-2" style="zoom:50%;" />

- 이와 같이 왼쪽 두 단어만을 맥락으로 생각했을 때 CBOW 모델이 출력할 확률은 다음과 같습니다.

<img src="README.assets/e 5-2.png" alt="e 5-2" style="zoom:50%;" />

#### Note

> word2vec에서 맥락의 윈도우 크기는 하이퍼파라미터로 임의의 값을 설정할 수 있습니다.
>
> 이번 예에서는 윈도우 크기를 '왼쪽 2, 오른쪽 0'처럼 좌우 비대칭으로 설정했습니다.
>
> 이렇게 설정한 이유는 나중에 '언어 모델'에서 이야기하겠습니다.

- 그런데 위 식 표기를 사용하면 CBOW 모델이 다루는 손실함수를 교차 엔트로피 오차에 의해 다음과 같이 유도할 수 있습니다.

<img src="README.assets/e 5-3.png" alt="e 5-3" style="zoom:50%;" />

- CBOW 모델의 학습으로 수행하는 일은 위 식의 손실 함수(정확히는 말뭉치 전체의 손실 함수의 총합)를 최소화하는 가중치 매개변수를 찾는 것입니다.

  - 이러한 가중치 매개변수가 발견되면 CBOW 모델을 맥락에서 타깃을 더 정확하게 추측할 수 있습니다.

- 이처럼 CBOW 모델을 학습시키는 목적은 맥락에서 타깃을 정확하게 추측하는 것으로 학습 부산물로 단어의 의미가 인코딩된 '단어의 분산표현'을 얻을 수 있습니다.

- 한편 CBOW 모델의 목적인 '맥락에서 타깃을 추측하는 것'은 어디에 이용될 수 있는가와 위 식의 확률의 실용적인 쓰임이 바로 '언어 모델'에서 나타나게 됩니다.

### 5.1.2 언어 모델

- **언어 모델**은 단어 나열에 확률을 부여합니다.

  - 특정 단어의 시퀀스에 대해서, 그 시퀀스가 일어날 가능성이 어느 정도인지 즉, 얼마나 자연스러운 단어 순서인지를 확률로 평가합니다.

  - 예를 들어 'you say goodbye'라는 단어 시퀀스는 높은 확률을, 'you say good die'는 낮은 확률을 출력하는 모델입니다.

- 이 언어 모델은 다양하게 응용되는데 기계 번역과 음성 인식이 대표적인 예입니다.

  - 음성 인식의 경우 사람의 음성에서 몇 개의 문장을 후보로 생성하고 언어 모델을 사용해 후보 문장이 '문장으로서 자연스러운지'를 기준으로 순서를 매깁니다.

  - 또한 언어 모델이 단어 순서의 자연스러움을 확률적으로 평가할 수 있으므로 새로운 문장을 생성하는 용도로도 이용할 수 있습니다.

    - 그 확률 분포에 따라 다음으로 적합한 단어를 '자아낼(샘플링)' 수 있기 때문으로 이는 '7장 RNN을 사용한 문장 생성'에서 배울 것입니다.

- 그러면 언어 모델을 수식으로 설명해보겠습니다. 여기서는 w<sub>m</sub>까지 m개의 단어로 된 문장을 생각해보겠습니다.

- 이 때 단어가 w<sub>1</sub>에서 w<sub>m</sub> 순서로 나타날 확률을 P(w<sub>1</sub>, ..., w<sub>m</sub>)이라고 할 수 있고 이는 동시에 일어나므로 동시 확률입니다. 이 동시 확률 P는 사후 확률을 사용해 다음과 같이 분해할 수 있습니다.

<img src="README.assets/e 5-4.png" alt="e 5-4" style="zoom:50%;" />

- 위 식에서 파이 기호는 모든 원소를 곱하는 '총곱'을 의미하며 동시 확률을 사후 확률의 총곱으로 나타낼 수 있습니다.

- 위 식의 경과는 확률의 **곱셈정리**에서 유도한 것으로 곱셈정리에 대한 간단한 설명 후에 이를 도출하는 과정을 설명하겠습니다. 우선 곱셈 정리는 다음과 같습니다.

<img src="README.assets/e 5-5.png" alt="e 5-5" style="zoom:50%;" />

- 위와 같은 곱셈정리는 확률론에서 가장 중요한 정리로 'A와 B가 모두 일어날 확률 P(A, B)는 B가 일어날 확률 P(B)에 B가 일어난 후 A가 일어날 확률 P(A|B)를 곱한 것과 같다.'는 것입니다.

#### Warning

> 확률 P(A, B)는 P(A, B) = P(B|A)P(A)로 분해할 수 있습니다.
>
> 즉, A와 B 중 어느 것을 사후 확률의 조건으로 할지에 따라 두 가지 식 표현 방법이 존재합니다.

- 이 곱셈정리를 사용하면 m개의 단어의 동시 확률을 사후 확률로 나타낼 수 있는데 이 때 수행하는 식 변형을 알기 쉽게 나타내면 다음과 같습니다.

<img src="README.assets/e 5-6.png" alt="e 5-6" style="zoom:50%;" />

- 여기에서 w<sub>m</sub>을 제외한 나머지를 A로 표현했는데 이 때 P(A) 역시 다음과 같은 식 변형을 할 수 있습니다.

<img src="README.assets/e 5-7.png" alt="e 5-7" style="zoom:50%;" />

- 이처럼 단어 시퀀스를 하나씩 줄여가면서 매번 사후 확률로 분해하고 이를 반복해 [식 5.4]를 이끌어낼 수 있습니다.

- 이 때 [식 5.4]에서 알 수 있듯이 목적으로 하는 동시 확률은 사후 확률의 총곱으로 대표할 수 있고 여기서 주목할 것은 이 사후 확률은 타깃 단어보다 왼쪽에 있는 모든 단어를 맥락(조건)으로 했을 때의 확률이라는 것입니다.

<img src="README.assets/fig 5-3.png" alt="fig 5-3" style="zoom:50%;" />

- 즉, 이를 정리하면 우리의 목표인 사후 확률을 구하는 것으로 이를 계산할 수 있다면 언어 모델의 동시 확률을 구할 수 있습니다.

#### Note

> P(w<sub>t</sub>|others)를 나타내는 모델은 **조건부 언어 모델**이라고 합니다.
>
> 한편 해당 확률을 나타내는 모델을 '언어 모델'이라 하는 경우도 많이 볼 수 있습니다.

### 5.1.3 CBOW 모델을 언어 모델로?

- 그렇다면 word2vec의 CBOW 모델을 언어 모델에 적용하는 방법은 무엇이 있을까요?

  - 이는 맥락의 크기를 특정 값으로 한정하여 근사적으로 나타낼 수 있는데 수식으로는 다음과 같습니다.

  <img src="README.assets/e 5-8.png" alt="e 5-8" style="zoom:50%;" />

  - 여기에서는 맥락을 왼쪽 두 개의 단어로 한정한 것으로 CBOW 모델에 따라(정확히는 모델의 사후 확률) 근사적으로 나타낼 수 있습니다.

#### Note

> 머신러닝이나 통계학에서는 **마르코프 연쇄** 또는 **마르코프 모델**이라는 말을 자주 듣습니다.
>
> 마르코프 연쇄란 미래의 상태가 현재 상태에만 의존해 결정되는 것을 의미합니다.
>
> 또한, 이 사상의 확률을 '그 직전' N개의 사건에만 의존할 때 이를 'N층 마르코프 연쇄'라고 합니다.
>
> 이번 예는 직전 2개의 단어에만 의존해 다음 단어가 정해지는 모델이므로 '2층 마르코프 연쇄라고 할 수 있습니다.

- 위 식에서는 맥락으로 두 개의 단어를 이용했지만 맥락의 크기는 임의의 길이로 설정할 수 있습니다.

- 하지만 결국 특정 길이로 '고정'되는데 예를 들어 왼쪽 10개의 단어를 맥락으로 하는 CBOW 모델은 그 맥락보다 더 왼쪽의 단어 정보를 무시합니다.

  - 이러한 경우 문제가 발생하는데 그 예가 다음과 같습니다.

<img src="README.assets/fig 5-4.png" alt="fig 5-4" style="zoom:50%;" />

- 위 예시의 문제의 맥락을 고려하면 정답은 'Tom(he)'이지만 이를 알기 위해 타깃보다 18번째 앞의 단어 'Tom'을 기억해야 합니다.

  - 따라서 10개의 맥락을 가지는 CBOW 모델은 이를 제대로 답할 수 없습니다.

- 그렇다고 CBOW 모델의 맥락 크기를 계속해서 키워나가도 CBOW 모델이 맥락 안의 단어 순서를 무시한다는 한계가 있습니다.

#### Note

> CBOW는 continuous bag-of-words의 약자로 bag-of-words는 '가방 안의 단어'를 의미하는데 이 때 단어의 '순서'는 무시된다는 뜻이 내포되어 있습니다.

- 맥락의 단어 순서가 무시되는 구체적인 예로 맥락으로 2개의 단어를 다루는 CBOW 모델이 두 개의 단어 벡터의 '합'이 은닉층으로 넘어오는 과정은 다음과 같습니다.

<img src="README.assets/fig 5-5.png" alt="fig 5-5" style="zoom:50%;" />

- 위 그림의 왼쪽과 같이 CBOW 모델의 은닉층에서는 단어 벡터들이 더해지므로 맥락의 단어 순서는 무시됩니다.

- 이상적으로는 맥락의 단어 순서도 고려한 모델이 바람직한데 이는 위 그림의 오른쪽 모델과 같습니다.

  - 오른쪽 모델은 단어 벡터를 은닉층에서 **연결**하는 방식을 사용하는데 실제 신경 확률론적 언어 모델에서 제안한 모델은 이 방식을 취합니다.

  - 그러나 연결하는 방식은 맥락의 크기에 비례해 가중치 변수가 늘어나는 단점을 불러옵니다.

- 따라서 이러한 문제를 해결하기 위해 순환 신경망, 즉 RNN이 등장했습니다.

- RNN은 맥락이 아무리 길더라도 그 맥락의 정보를 기억하는 매커니즘을 갖추고 있습니다. 그래서 RNN을 사용하면 아무리 긴 시계열 데이터에도 대응할 수 있습니다.

#### Warning

> word2vec은 단어의 분산 표현을 얻을 목적으로 고안된 기법으로 이를 언어 모델로 사용하는 경우는 보통 잘 없습니다.
>
> 여기에서는 RNN의 매력을 알아가기 위해 word2vec의 CBOW 모델을 억지고 언어 모델에 적용했을 뿐입니다.
>
> 특히 word2vec의 RNN보다 늦게 제안되었는데 RNN 언어 모델이 단어의 분산 표현을 얻을 수 있지만 어휘 수 증가에 따른 대응이나 단어의 분산 표현의 '질' 개선을 위해 word2vec을 제안되었습니다.

## 5.2 RNN이란

- RNN은 'Recurrent'라는 라틴어에서 온 말로, '몇 번이나 반복해서 일어나는 일'을 의미합니다. 그래서 RNN을 직역하면 '순환하는 신경망'으로 이번 절에서는 '순환한다'의 의미를 짚어보겠습니다.

#### Warning

> Recurrent Neural Network는 우리말로 '순환 싱경망'이라고 번역합니다.
>
> 반면 Recursive Neural Network라는 '재귀 신경망'도 있는데 이는 주로 트리 구조로 데이터를 처리하기 위한 신경망입니다.

### 5.2.1 순환하는 신경망

- '순환하다'의 의미는 '반복해서 되돌아감'을 의미합니다. 즉, 어느 한 지점에서 시작한 것이 시간을 지나 원래 장소로 돌아오는 것과 이를 반복하는 것이 바로 '순환'입니다.

- 여기서 주목할 점은 순환하기 위해서는 '닫힌 경로'가 필요하다는 점입니다.

  - '닫힌 경로' 혹은 '순환하는 경로'가 존재해야 데이터가 같은 장소를 반복해 왕래할 수 있고 그 결과 정보가 끊임없이 갱신되게 됩니다.

#### Note

> 비유하자면 우리의 체내를 순환하는 혈액은 그 생명을 얻은 순간부터 계속해 흘러 체내를 순환하며 과거에서 현재까지 끊임없이 '갱신'됩니다.

- RNN의 특징은 순환 경로(닫힌 경로)가 있다는 것으로 이 경로를 따라 데이터가 끊임없이 순환합니다.

  - 그리고 데이터가 순환되기에 과거의 정보를 기억하는 동시에 최신 데이터로 갱신될 수 있는 것입니다.'

- 그럼 RNN을 구체적으로 살펴보겠습니다. RNN에 이용되는 계층을 'RNN 계층'이라고 할 때 그는 다음과 같습니다.

<img src="README.assets/fig 5-6.png" alt="fig 5-6" style="zoom:50%;" />

- 이처럼 RNN 계층은 순환하는 경로를 포함하는데 이 순환 경로에 따라 데이터를 계층 안에서 순환시킬 수 있습니다.

  - RNN 계층은 x<sub>t</sub>을 입력받는데 이는 시간 t에 대한 데이터로 이들을 시계열 데이터라고 합니다.

  - 그리고 이에 대응해서 h<sub>t</sub>가 다음 층으로 출력됩니다

- 또한 각 시각에 입력되는 x<sub>t</sub>가 벡터라고 할 때 문장을 다루는 경우 각 단어의 분산 표현이 입력이 되고 이들이 순서대로 하나씩 RNN 계층에 입력됩니다.

#### Warning

> 위 그림에 따르면 출력이 2개로 분기하는데 이 분기되는 데이터들은 같은 데이터가 복제되어 '분기'함을 의미합니다.
>
> 그리고 그 중 하나가 RNN 계층에 다시 입력됩니다.

- 이어서 순환 구조를 자세히 살펴보기 전에 RNN 계층을 다음과 같이 그리겠습니다.

<img src="README.assets/fig 5-7.png" alt="fig 5-7" style="zoom:50%;" />

- 이처럼 지금까지 데이터가 왼쪽에서 오른쪽으로 흐르는 형태의 계층과 달리 아래에서 위로 흐르는 계층으로 그리겠습니다.

### 5.2.2 순환 구조 펼치기

- 그럼 RNN 계층의 순환 구조에 대해 자세하게 살펴보겠습니다.

- RNN의 순환 구조는 지금까지 신경망에 없었던 구조지만 이 구조를 펼치면 친숙한 신경망으로 '변신'합니다.

<img src="README.assets/fig 5-8.png" alt="fig 5-8" style="zoom:50%;" />

- 위 그림과 같이 RNN 계층의 순환 구조를 펼치면 오른쪽으로 성장하는 긴 신경망을 볼 수 있습니다. 그리고 이는 지금까지 본 피드포워드 신경망과 같은 구조입니다.

  - 다만, 등장하는 RNN 계층 모두가 사실 '같은 계층'이라는 것만 지금까지 신경망과 다릅니다.

#### Warning

> 시계얼 데이터는 시간 방향으로 데이터가 나열됩니다.
>
> 따라서 시계열 데이터의 인덱스를 가리킬 때는 '시각'이라는 용어를 사용합니다.
>
> 이는 자연어에서도 't번째 단어 혹은 RNN 계층'말고도 '시각 t의 단어 혹은 RNN 계층'이라고 표현합니다.

- 이처럼 각 시각의 RNN 계층은 그 계층의 입력과 1개 전의 RNN 계층의 출력을 받고 두 정보를 바탕으로 현 시각의 출력을 계산합니다. 그 수식은 다음과 같습니다.

<img src="README.assets/e 5-9.png" alt="e 5-9" style="zoom:50%;" />

- 위 수식을 보면 RNN에 사용되는 가중치는 두 개인데 하나는 입력 x를 출력 h로 바꾸는 가중치 W<sub>x</sub>이고 다른 하나는 1개 전의 출력을 다음 시각의 출력으로 변환하기 위한 W<sub>h</sub>입니다.

  - 또한 편향 b도 존재하며 h<sub>t-1</sub>과 x<sub>t</sub>는 행벡터입니다.

- 위 수식은 행렬곱을 계산하고 그 합을 ranh 함수를 이용해 변환하여 시각 t의 출력 h<sub>t</sub>을 계산합니다. 그리고 이 출력은 다음 계층과 다음 시각의 RNN 계층을 향해 출력됩니다.

- 그런데 위 수식을 보면 현재의 출력은 한 시각 이전의 출력에 기초해 계산됨을 알 수 있는데 다른 관점으로 이를 보면, RNN이 h라는 '상태'를 가지고 있고 이를 위 수식을 사용해 갱신한다고 할 수 있습니다.

  - 그래서 RNN 계층을 '상태를 가지는 계층' 혹은 '메모리가 있는 계층'이라고 합니다.

#### Note

> RNN의 h는 '상태'를 기억해 시각이 1 단위 진행될 때마다 위 식에 의해 갱신됩니다.
>
> 많은 문헌에서 RNN의 출력 h<sub>t</sub>을 **은닉 (벡터) 상태**라고 부르며 이 책에서도 동일하게 부릅니다.

- 또 펼처진 RNN 계층을 많은 문헌에서 다음과 같이 그리기도 합니다.

<img src="README.assets/fig 5-9.png" alt="fig 5-9" style="zoom:50%;" />

- 왼쪽 그림은 RNN 계층에서 나가는 두 화살표가 같은 데이터가 복사되어 분기됨을 확인하기 어렵습니다.

- 따라서 이 책에서는 지금처럼 오른쪽 그림과 같이 하나의 출력이 분기하는 그림을 사용해 명시하겠습니다.

### 5.2.3 BPTT

- 앞에서 봤듯이 RNN 계층은 가로로 펼친 신경망으로 간주할 수 있으며 학습도 보통의 신경망과 같은 순서로 진행할 수 있습니다.

<img src="README.assets/fig 5-10.png" alt="fig 5-10" style="zoom:50%;" />

- 위 그림처럼 순환 구조를 펼친 후의 RNN에는 일반적인 오차역전파법을 적용할 수 있습니다.

  - 즉, 먼저 순전파를 수행하고, 이어서 역전파를 수행해 원하는 기울기를 구할 수 있습니다.

  - 여기서 오차역전파법은 '시간 방향으로 펼쳐진 신경망의 오차역전파법'이라는 뜻으로 **BPTT**라고 합니다.

- 이 BRTT를 사용하면 RNN을 학습할 수 있을 듯 보이지만 해결해야할 문제가 하나 있는데 그것은 바로 긴 시계열 데이터를 학습할 때 발생합니다.

  - 이는 시계열 데이터의 시간 크기가 커지는 것에 비례해 BPTT가 소비하는 컴퓨팅 자원도 증가하기 때문입니다.

  - 또한, 시간 크기가 커지면 역전파의 기울기가 불안정해지는 것도 문제입니다.

#### Note

> BPTT를 이용해 기울기를 구하려면 매 시각 RNN 계층의 중간 데이터를 메모리에 유지해야 합니다.
>
> 따라서 시계열 데이터가 길어짐에 따라 계산량과 메모리 사용량이 증가합니다.

### 5.2.4 Truncated BPTT

- 큰 시계열 데이터를 취급할 때 흔히 신경망 연결을 적당한 길이로 '끊습니다'.

  - 시간축 방향으로 너무 길어진 신경망을 적당한 지점에서 잘라 작은 신경망 여러 개로 만드는 아이디어로 이 작은 신경망들에 대해 오차역전파법을 수행합니다.

  - 이것이 바로 **Truncated BPTT** 기법입니다.

#### Note

> Truncated는 '잘린'이라는 뜻이므로 Truncated BPTT는 적당한 길이로 '잘라낸' 오차역전파법입니다.

- Truncated BPTT는 신경망의 연결을 끊지만 제대로 구현하기 위해서는 '역전파'의 연결만 끊고 순전파는 유지해야 순전파의 흐름이 끊어지지 않고 전파됩니다.

  - 한편, 역전파의 연결은 적당한 길이오 잘라내 그 잘라낸 신경망 단위로 학습을 수행합니다.

- 구체적인 예와 함께 살펴보자면 길이가 1000개인 시계열 데이터(자연어 처리라면 단어 1000개의 말뭉치)가 있다고 가정하겠습니다.

  - 지금껏 다룬 PTB 데이터셋도 여러 문장을 연결한 것을 하나의 큰 시계열 데이터로 취급했는데 여기에서도 마찬가지로 데이터를 취급하겠습니다.

- 그런데 길이가 1000인 시계열 데이터를 다루는 RNN 계층을 펼치면 가로로 1000개나 늘어선 신경망이 됩니다.

  - 아무리 계층이 늘어서더라도 오차역전파법으로 기울기를 계산할 수는 있지만 너무 길면 계산량과 메모리 사용량 등에서 문제가 발생합니다.

  - 또한, 길어진 계층에 의해 신경망을 하나 통과할 때마다 기울기 값이 조금씩 작아져 이전 시각 t까지 역전파되기 전에 0이 되어 소멸할 수 있습니다.

- 바로 이런 이유로 길게 뻗은 신경망의 역전파는 연결을 적당한 길이로 끊을 생각을 한 것입니다.

<img src="README.assets/fig 5-11.png" alt="fig 5-11" style="zoom:50%;" />

- 위 예시에서는 RNN 계층을 10개 단위로 학습할 수 있도록 역전파의 연결을 끊었습니다.

- 이처럼 역전파의 연결을 잘라버리면 그보다 미래의 데이터에 대해서는 생각할 필요가 없어집니다.

  - 따라서, 각 블록 단위로 미래의 블록과 독립적으로 오차역전파법을 완결시킬 수 있습니다.

- 여기서 기억할 점은 역전파의 연결은 끊어지지만 순전파의 연결은 그렇지 않다는 점입니다.

  - 그러므로 RNN을 학습시킬때는 순전파가 연결된다는 점을 고려해야 합니다.

  - 즉, 데이터를 '순서대로' 입력해야 한다는 뜻인데 이에 대해서는 이어서 자세히 설명하겠습니다.

#### Warning

> 지금까지 본 신경망은 미니배치 학습을 수행할 때 데이터를 무작위로 선택해 입력했습니다.
>
> 하지만 RNN에서 Truncated BPTT를 수행할 때는 데이터를 '순서대로' 입력해야 합니다.

- 이제 Truncated BPTT 방식으로 RNN을 학습시켜 보겠습니다.

  - 가장 먼저 할 일은 첫 번째 블록 입력 데이터를 RNN 계층에 제공하는 것입니다.

  <img src="README.assets/fig 5-12.png" alt="fig 5-12" style="zoom:50%;" />

  - 이처럼 순전파를 먼저 수행하고 다음에 역전파를 수행해 원하는 기울기를 구할 수 있습니다. 이어서 다음 블록의 입력 데이터를 입력해 오차역전파법을 수행합니다.

  <img src="README.assets/fig 5-13.png" alt="fig 5-13" style="zoom:50%;" />

  - 여기도 첫 번째 블록과 마찬가지로 순전파를 수행하고 역전파를 수행합니다. 그리고 여기서 중요한 점은 이번 순전파 계산에는 앞 블록의 마지막 은닉 상태의 정보가 필요하다는 점으로 이를 통해 순전파는 계속 연결됩니다.

  - 같은 방식으로 세 번째 블록을 대상으로 학습을 수행하는데 이 때도 두 번째 블록의 마지막 은닉 상태 정보를 이용합니다.

  - 이처럼 RNN 학습은 데이터를 순서대로 입력하며 은식 상태를 계승하면서 학습을 수행합니다. 이들을 총망라하면 다음과 같습니다.

  <img src="README.assets/fig 5-14.png" alt="fig 5-14" style="zoom:50%;" />

- 이와 같이 Truncated BPTT에서 데이터를 순서대로 입력해 학습할 수 있고 순전파의 연결을 유지하면서 블록 단위로 오차역전파법을 적용할 수 있습니다.

### 5.2.5 Truncated BPTT의 미니배치 학습

- 지금까지 Truncated BPTT에서는 미니배치 학습 시 각 미니배치가 어떤 식으로 이루어지는지에 대해 생각하지 않았습니다.

  - 굳이 말해 지금까지 이야기는 미니배치가 1일 때에 해당되는데 이제 미니배치 학습을 위해 구체적인 배치 방식을 고려해 데이터를 순서대로 입력해야 합니다.

  - 그렇게 하기 위해서는 데이터를 주는 시작 위치를 각 미니배치의 시작 위치로 '옮겨줘야' 합니다.

- '옮긴다'는 의미를 앞서 설명한 예(길이 1000인 시계열 데이터와 10 시각 단위로 자른 Truncated BPTT)로 설명하겠습니다.

  - 만일 미니배치를 두 개로 구성한가면 RNN 계층의 입력 데이터로 첫 번째 미니배치 때는 처음부터 순서대로 데이터를 제공합니다.

  - 그리고 두 번째 미니배치 때는 500번째 데이터를 시작 위치로 정하고 그 위치부터 다시 순서대로 데이터를 제공합니다. 이들은 다음과 같습니다.

  <img src="README.assets/fig 5-15.png" alt="fig 5-15" style="zoom:50%;" />

- 이와 같이 미니배치 데이터를 RNN의 입력 데이터로 사용해 학습을 수행합니다. 이후로는 순서대로 진행되므로 다음에 넘길 데이터는 각 시계열 데이터의 10 시각 다음 데이터가 되는 형식입니다.

- 이처럼 미니배치 학습을 수행할 때 각 미니배치의 시작 위치를 오프셋으로 옮겨준 후 순서대로 제공하면 됩니다. 또한, 데이터를 순서대로 입력하다가 끝에 도달하면 다시 처음부터 입력하도록 합니다.

- 지금까지 살펴본 Truncated BPTT의 원리는 단순하지만 '데이터 제공 방법'면에서는 몇 가지 주의가 필요합니다.

  - 구체적으로는 '데이터 순서대로 제공하기'와 '미니배치별로 데이터 제공하는 시작위치를 옮기기'입니다.

  - 이에 대한 설명이 복잡해 잘 이해되지 않을수도 있지만 이어서 소스 코드와 함께 동작을 지켜보면 어렵지 않게 이해할 수 있을 것입니다.
