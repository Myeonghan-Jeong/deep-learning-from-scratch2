# chapter03. word2vec

- 앞 장에 이어 이번 장의 주제도 단어의 분산 표현이지만 앞 장에서 '통계 기반 기법'을 사용한 것과 달리 더 강력한 '추론 기반 기법'을 사용할 것입니다.

- 이름에서 보듯 '추론 기반 기법'은 추론을 하는 기법으로 신경망을 사용하는데 그 신경망이 word2vec입니다.

  - 이번 장에서는 word2vec의 구조를 알아보고 이를 직접 구현해보며 확실하게 이해할 것입니다.

- 이번 장의 목표는 단순한 word2vec 구현하기로 처리 효율을 희생한 대신 이해하기 쉬운 word2vec입니다.

  - 따라서 큰 데이터셋에서는 어렵지만 작은 데이터셋은 문제없이 처리할 수 있습니다.

  - 또한, 다음 장에서 이 word2vec에 여러 개선사항을 더해 '진짜' word2vec을 구현할 예정입니다.

## 3.1 추론 기반 기법과 신경망

- 단어를 벡터로 표현하는 방법 중 가장 성공적인 기법은 크게 '통계 기반 기법'과 '추론 기반 기법'입니다.

  - 이 둘은 단어의 의미를 얻는 방식이 서로 크게 다르지만 그 배경에는 모두 분포 가설이 있습니다.

- 이번 절에서는 통계 기반 기법의 문제를 지적하고 그 대인아니 추론 기반 기법의 이점을 거시적인 관점에서 설명합니다.

  - 또한 word2vec의 전처리를 위한 신경망으로 '단어'를 처리하는 예시를 살펴볼 것입니다.

### 3.1.1 통계 기반 기법의 문제점

- 지금까지 본 것처럼 통계 기반 기법은 주변 단어의 빈도를 기초로 단어를 표현합니다.

  - 구체적으로 단어의 동시발생 행렬을 만들고 그 행렬에 SVD를 적용하여 밀집 벡터로 재구성합니다. 하지만 이 방식은 대규모 말뭉치를 다룰 때 문제가 발생합니다.

- 현실에서 다루는 말뭉치의 어휘 수는 어마어마합니다. 예컨대 영어의 어휘 수는 100만을 훌쩍 넘는데 어휘가 100만 개라면 통계 기반 기법에서는 그 제곱의 행렬을 만들게 됩니다. 이는 비현실적입니다.

#### Note

> SVD를 n by n 행렬에 적용하는 비용은 O(n<sup>3</sup>)입니다.
>
> 이는 계산 시간이 n의 세제곱에 비례한다는 뜻으로 슈퍼컴퓨터를 동원해도 처리하기 힘든 수준입니다.
>
> 실제로 근사적 기법과 희소행렬의 성질 등으로 이를 개선할 수 있지만 그렇다고 해도 여전히 상당한 컴퓨팅 자원을 소모합니다.

- 통계 기반 기법은 말뭉치 전체의 통계를 이용해 단 1회의 처리만에 단어의 분산 표현을 얻습니다.

- 한편, 추론 기반 기법에서는 신경망을 이용한다면 미니배치로 학습하는 것이 일반적입니다.

  - 미니배치 학습에서는 신경망이 한 번에 소량의 학습 샘플씩 반복해서 학습하여 가중치를 갱신해갑니다.

  <img src="README.assets/fig 3-1.png" alt="fig 3-1" style="zoom:50%;" />

- 위의 그림처럼 통계 기반 기법은 학습 데이터를 한 번에 처리합니다.(배치 학습) 반면에 추론 기반 기법은 학습 데이터의 일부를 사용해 순차적으로 학습합니다.(미니배치 학습)

  - 이는 말뭉치의 어휘 수가 많아 SVD 등 계산량이 큰 작업을 처리하기 어려운 경우에도 신경망을 학습시킬 수 있다는 것으로 데이터를 작게 나눠 학습시킵니다.

  - 게다가 여러 GPU를 사용한 병렬 계산도 가능하므로 학습 속도를 높일 수도 있습니다.

  - 추론 기반 기법이 통계 기반 기법보다 매력적인 점은 이 외에도 여러 가지가 있으며 이는 추론 기반 기법을 학습한 뒤 '3.5.3 통계 기반 vs 추론 기반'에서 알아보겠습니다.

### 3.1.2 추론 기반 기법 개요

- 추론 기반 기법은 '추론'이 주된 작업인데 추론은 주변 단어가 주어졌을 때 빈 칸에 들어갈 단어를 추측하는 작업입니다.

<img src="README.assets/fig 3-2.png" alt="fig 3-2" style="zoom:50%;" />

- 이처럼 추론 문제를 풀고 학습하는 것이 '추론 기반 기법'이 다루는 문제로 이 문제를 반복해서 풀면서 단어의 출현 패턴을 학습합니다.

  - 이를 '모델 관점'에서 바라보면 다음과 같습니다.

<img src="README.assets/fig 3-3.png" alt="fig 3-3" style="zoom:50%;" />

- 이처럼 추론 기반 기법에는 어떤 모델이 등장하는데 이 모델로 신경망을 사용할 것입니다.

  - 모델은 맥락의 정보를 입력받아 출현할 수 있는 각 단어의 출현 확률을 출력하고 그 안애서 말뭉치를 사용해 올바른 추측을 내릴 수 있도록 학습합니다.

  - 그리고 그 학습의 결과로 단어의 분산 표현을 얻는 것이 추론 기반 기법의 전체 그림입니다.

#### Note

> 추론 기반 기법도 통계 기반 기법처럼 분포 가설이 존재합니다.
>
> 분포 가설이란 '단어의 의미는 주변 단어에 의해 형성된다'는 가설로 추론 기반 기법에서는 이를 앞과 같은 추측 문제로 귀결시켰습니다.
>
> 이처럼 두 기법 모두 분포 가설에 근거하는 '단어의 동시발생 가능성'을 얼마나 잘 모델링하는가가 중요한 연구입니다.

### 3.1.3 신경망에서의 단어 처리

- 지금부터 신경망을 이용해 단어를 처리합니다만 신경망은 단어를 그래도 처리할 수 없으니 단어를 '고정 길이의 벡터'로 변환합니다. 이 방법이 **원핫 표현(벡터)**입니다.

  - 원핫 표현이란 벡터의 원소 중 하나만 1이고 나머지는 0인 벡터를 의미합니다.

- 원핫 표현은 자세하게 다음과 같이 'You say goodbye and I say hello'라는 문장이 있을 때 이를 다음과 같이 처리합니다.

<img src="README.assets/fig 3-4.png" alt="fig 3-4" style="zoom:50%;" />

- 이처럼 단어는 텍스트, 단어 ID 그리고 원핫 표현 형태로 나타낼 수 있고 단어를 원핫 표현으로 변환하는 방법은 다음과 같습니다.

  - 먼저 총 어휘 수만큼 원소를 갖는 벡터를 준비하고 인덱스가 단어 ID와 같은 원소를 1로 나머지를 0으로 설정합니다.

  - 그럼 단어를 고정 길이 벡터로 변환하면 신경망 입력층은 뉴런의 수를 고정할 수 있습니다.

<img src="README.assets/fig 3-5-1585116923977.png" alt="fig 3-5" style="zoom:50%;" />

- 위의 예시처럼 입력층의 뉴런은 총 7개이고 각 뉴런은 각 단어에 대응합니다. 이제 이를 사용해 단어를 벡터로 나타낼 수 있고 신경망을 구성하는 '계층'들을 벡터로 처리할 수 있습니다.

  - 다시 말해, 단어를 신경망으로 처리할 수 있다는 뜻으로 다음처럼 원핫 표현의 단어 하나를 완전연결계층을 통해 변환할 수 있습니다.

<img src="README.assets/fig 3-6.png" alt="fig 3-6" style="zoom:50%;" />

- 위 신경망은 완전연결계층이므로 각 노드가 이웃 층의 모든 노드와 연결되어 있습니다.

  - 이 화살표에는 가중치(매개변수)가 존재하며 입력층 뉴런과 가중치의 합이 은닉층 뉴런이 됩니다. 참고로 이번 장의 완전연결 계층에서는 편향을 생략합니다.

#### Note

> 편향을 이용하지 않는 완전연결계층은 '행렬 곱' 연산에 해당합니다.
>
> 그래서 이 책에서 완전연결계층은 1장에서 구현한 MatMul 계층과 같아집니다.
>
> 참고로 딥러닝 프레임워크들은 일반적으로 완전연결계층을 생성할 때 편향을 이용할지 선택할 수 있습니다.

- 위에서 본 그림에서 가중치를 명확히 표현하기 위해 바꿔 그리면 다음과 같습니다.

<img src="README.assets/fig 3-7.png" alt="fig 3-7" style="zoom:50%;" />

- 그럼 이를 코드로 살펴보고 넘어가겠습니다. 거두절미하고 이 완전연결 계층에 의한 변환은 python으로 다음과 같습니다.

```python
import numpy as np

c = np.array([[1, 0, 0, 0, 0, 0, 0]])  # 입력
W = np.random.randn(7, 3)  # 가중치
h = np.matmul(c, W)  # 중간 노드
print(h)
```

- 이 코드는 단어가 0인 단어를 원핫 표현으로 표현하고 완전연결계층을 통과시켜 변환하는 모습입니다.

  - 복습해보자면, 완전연결계층의 계산은 행렬곱으로 수행할 수 있고 이 연산은 np.matmul()이 해결합니다.

#### Warning

> 이 코드에서 입력 데이터 c의 차원 수(ndim)은 2입니다.
>
> 이는 미니배치 처리를 고려한 것으로 최초의 차원(0번째 차원)에 각 데이터를 저장합니다.

- 앞선 코드에서 주목할 부분은 c와 W의 행렬 곱 부분으로 c는 원핫 표현이라서 단어 ID에 대응하는 원소만 1이고 나머지는 0인 벡터입니다. 따라서 앞선 c와 W의 연산은 결국 가중치의 행벡터 하나를 '뽑아낸' 것과 같습니다.

<img src="README.assets/fig 3-8.png" alt="fig 3-8" style="zoom:50%;" />

- 그저 가중치에 행벡터 하나를 뽑아낼 뿐인데 행렬 곱을 계산하는 건 비효율적으로 보일 수 있는데 이는 '4.1 word2vec의 개선 1'에서 개선할 예정입니다. 다음처럼.

```python
from commons.layers import MatMul

c = np.array([[1, 0, 0, 0, 0, 0, 0]])
W = np,random.randn(7, 3)

layer = MatMul(W)
h = layer.forward(c)
print(h)
```

- 위 코드는 commons 디렉토리 안에 있는 MatMul 계층을 import하여 사용합니다. 그런 다음 해당 계층에 가중치 W를 설정하고 forward를 사용해 순전파를 계산합니다.
